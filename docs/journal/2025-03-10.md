# 📅 2025-02-25 - Výzkum a implementace algoritmů pro kompresi stromových struktur

Dnes jsem se zaměřil na implementaci a výzkum různých existujících algoritmů pro kompresi stromových struktur. Prozkoumal jsem tři hlavní algoritmy: **DictionaryTreeCompression**, **FrequentSubtreeCompression** a **RePairTreeCompressor**. Každý z těchto algoritmů má své výhody a specifické využití v závislosti na druhu dat, která se komprimují. Tento výzkum mi pomohl lépe pochopit, jak každý z těchto přístupů funguje a jak je možné je využít pro optimalizaci komprese stromů ve svém projektu.

## 📚 Algoritmy pro kompresi stromových struktur

### 1. **DictionaryTreeCompression** 
Tento algoritmus je známý svou efektivitou při kompresi dat ve stromových strukturách, přičemž je běžně používán pro **kompresi XML dat** a dalších hierarchických datových formátů. Funguje tak, že vytváří slovník, který obsahuje opakující se podstromy. Každý podstrom je reprezentován klíčem ve slovníku, což umožňuje kompresi tím, že místo opakovaných podstromů se používá pouze jejich klíč. Tento přístup výrazně snižuje velikost dat, zejména pokud existují rekurentní vzory.

**Možnosti využití:**
- Tento algoritmus bych mohl použít pro kompresi stromových struktur, které vykazují vysokou míru opakování v jejich podstrukturoch.
- V budoucnu by se mohl ukázat jako efektivní pro kompresi syntaktických stromů, pokud se budou vyskytovat opakující se vzory, například v dlouhých větách nebo textových blocích.

### 2. **FrequentSubtreeCompression**
Tento algoritmus se zaměřuje na **kompresi častých podstromů**, což znamená, že hledá podstromy, které se vyskytují často v celém stromu, a nahrazuje je jedinečnými identifikátory. Tento přístup je často používán v **bioinformatice** pro kompresi dat, jako jsou filogenetické stromy, a v dalších oblastech, kde se často opakují určité struktury.

**Možnosti využití:**
- Tento algoritmus by mohl být užitečný, pokud budu pracovat s rozsáhlými daty, kde některé podstromy nebo vzory struktury stromu mohou být velmi časté.
- V budoucnu se ukáže jako vhodný pro kompresi složitějších stromových struktur s vysokou mírou opakování, což je typické pro texty s mnoha podobnými větami.

### 3. **RePairTreeCompressor**
**RePair** je algoritmus, který se zaměřuje na **nalezení opakujících se vzorců** v datech a jejich nahrazení symboly, což vede k výrazné redukci velikosti. Tento algoritmus je oblíbený pro kompresi textů a **XML dat** a je známý svou efektivitou při hledání a nahrazování opakujících se podstruktur.

**Možnosti využití:**
- RePair je vhodný pro situace, kdy je potřeba efektivně komprimovat velké množství dat, a to zejména když se v datech nachází podobné podstruktury.
- Tento algoritmus by mohl být jedním z klíčových nástrojů pro mojí implementaci kompresního algoritmu, zejména pokud budu mít problém s velkým množstvím opakujících se vzorců v syntaktických stromech.

## 🛠️ Implementace a experimenty

V rámci implementace jsem se nejprve zaměřil na základní verzi každého algoritmu:

- **DictionaryTreeCompression**: Začal jsem implementací jednoduchého slovníku pro ukládání opakujících se podstromů. Testoval jsem ho na několika příkladech textu, kde jsem hledal opakující se fráze.
  
- **FrequentSubtreeCompression**: Tento algoritmus jsem implementoval tak, že jsem prohledával strom a identifikoval podstromy, které se vyskytovaly častěji než ostatní. Tyto podstromy jsem nahradil identifikátory.

- **RePairTreeCompressor**: Tento algoritmus jsem implementoval s využitím principu iterativní komprese, kde se v každé iteraci hledají a nahrazují opakující se vzory. Implementace vyžadovala dostatečně efektivní způsob, jak zpracovávat a ukládat nalezené vzory.

## 🚀 Výsledky a zhodnocení

Během implementace jsem se hodně naučil o těchto algoritmech a jejich výhodách:

- **DictionaryTreeCompression** se osvědčil jako efektivní pro kompresi textových stromů, ale je méně efektivní při zpracování stromů s nižšími mírami opakování.
- **FrequentSubtreeCompression** je velmi silný při kompresi dat, kde se vyskytují časté vzory. Může být užitečný pro struktury s výrazným opakováním.
- **RePairTreeCompressor** se ukázal jako nejlepší pro moji aplikaci, protože je schopný najít opakující se vzory a komprimovat je velmi efektivně, zejména u velkých stromů.

### 💡 Co dál?
V budoucnu bych chtěl:

- **Porovnat výkon** těchto algoritmů na reálných datech.
- **Vytvořit hybridní metodu**, která by kombinovala výhody jednotlivých algoritmů.
- **Testovat na větších datech**, abych zjistil, jak se chovají při vyšší složitosti a větší velikosti stromu.

Celkově jsem se naučil hodně o tom, jak algoritmy pro kompresi stromů fungují a jak je lze aplikovat na různé typy dat. Zatím se mi nejvíce osvědčil **RePair**, ale stále je prostor pro optimalizace a zlepšení.